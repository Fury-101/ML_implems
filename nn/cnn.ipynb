{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def forward():\n",
    "        pass\n",
    "    def backward():\n",
    "        pass\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.biases = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.biases\n",
    "\n",
    "    def backward(self, output_gradient, lr):\n",
    "        weights_gradient = np.dot(output_gradient, np.transpose(self.input))\n",
    "        input_gradient = np.dot(np.transpose(self.weights), output_gradient)\n",
    "        \n",
    "        self.weights -= lr * weights_gradient\n",
    "        self.biases -= lr * output_gradient\n",
    "        return input_gradient\n",
    "\n",
    "class Conv2D(Layer):\n",
    "                       #(depth, width, height) \n",
    "    def __init__(self, input_shape, kernel_size, num_kernels):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (num_kernels, input_shape[2] - kernel_size + 1, input_shape[1] - kernel_size + 1)\n",
    "        self.kernel_shape = (num_kernels, input_shape[0], kernel_size, kernel_size)\n",
    "        self.kernels = np.random.random_sample(self.kernel_shape)\n",
    "        self.biases = np.random.random_sample(self.output_shape)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        self.input = inp\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.kernel_shape[0]):\n",
    "            for j in range(self.input_shape[0]):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i][j], 'valid')\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, lr):\n",
    "        dCdK = np.zeros(self.kernel_shape)\n",
    "        dCdX = np.zeros(self.input_shape)\n",
    "        dCdB = np.zeros(output_gradient.shape)\n",
    "\n",
    "        for i in range(self.kernel_shape[0]):\n",
    "            dCdB[i] = output_gradient[i]\n",
    "            for j in range(self.input_shape[0]):\n",
    "                dCdK[i][j] = signal.correlate2d(self.input[j], output_gradient[i], 'valid')\n",
    "                dCdX[j] += signal.convolve2d(output_gradient[i], self.kernels[i][j], 'full')\n",
    "        \n",
    "        self.kernels -= lr * dCdK\n",
    "        self.biases -= lr * dCdB\n",
    "\n",
    "        return dCdX\n",
    "\n",
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return np.reshape(inp, self.output_shape)\n",
    "    \n",
    "    def backward(self, output_gradient, lr):\n",
    "        return np.reshape(output_gradient, self.input_shape)\n",
    "    \n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_gradient):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_gradient\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(self.sigmoid, self.gradient)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y, y_pred):\n",
    "    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_gradient(y, y_pred):\n",
    "    return ((1 - y) / (1 - y_pred) - y / y_pred) / np.size(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
